{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-collapse",
   "metadata": {},
   "source": [
    "## Exercise 5 \n",
    "\n",
    "Reinforcement learning: the game of pong with proximal policy optimisation.\n",
    "\n",
    "The goals are:\n",
    "1. understand how reinforcement learning connects with the neural networks we saw in previous exercises\n",
    "2. understand the concpet of a \"policy\" network\n",
    "3. understand the data collection/training loop\n",
    "\n",
    "<b> In this exercise you have to understand what is going on and build a training loop and let the agent score at least 5 points. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af881b91",
   "metadata": {},
   "source": [
    "### Loading the environment \n",
    "\n",
    "The position of the paddles and the ball define the state of our enviroment. The agent recieves the state as the input and it outputs the action to take. The action is taken and the state changes (giving a reward to the agent). This cycle repeats itself!\n",
    "\n",
    "The enviroment will give a feedback to the agent (+1 if the player scores, -1 if the computer scores, or 0).\n",
    "\n",
    "<img src=\"RL_architecture.jpeg\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m atari_py.import_roms roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7bbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae3efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24085793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of all the actions, we will assume only RIGHT and LEFT are relevant!\n",
    "\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7434056",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "screens = []\n",
    "\n",
    "for t in range(190000):\n",
    "    \n",
    "    next_state, reward, done, info = env.step(env.action_space.sample())\n",
    "    screens.append(next_state)\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a53e2b",
   "metadata": {},
   "source": [
    "The following cells are for creating the animation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ims = []\n",
    "\n",
    "for i, screen in enumerate( screens ):\n",
    "    if i % 5 !=0:\n",
    "        continue\n",
    "    im = ax.imshow(screen,animated=True)\n",
    "    if i == 0:\n",
    "        ax.imshow(screen)\n",
    "    ims.append([im])\n",
    "\n",
    "    if i > 5000:\n",
    "        break\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=20, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-lafayette",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "We want to cut away all the uneeded information from the screen, turn the image into a binary image only showing the paddles and ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0c197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PreProcess, PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = PreProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(6,3),dpi=120)\n",
    "\n",
    "idx = 1892\n",
    "ax[0].imshow(screens[idx])\n",
    "ax[1].imshow( preprocess(screens[idx]) )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc69153",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(screens[idx]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45474ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(screens[idx]).view(-1).unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d44837",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "The network will take 2 states (the current and the previous to give some sense of motion) and output the logits, the numbers that represent the probablity to pick between the two actions (left and right).\n",
    "\n",
    "We give 2 states to give a sense of motion to the neural network (where the ball is going). From the output we generate a categorical distribution, a discrete probability distribution that describes the possible results of a random variable that can take on one of K possible categories, with the probability of each category separately specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59c8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PolicyNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8446b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = preprocess(screens[idx]).view(-1).unsqueeze(0)\n",
    "previous_state = preprocess(screens[idx-1]).view(-1).unsqueeze(0)\n",
    "\n",
    "# Logits\n",
    "net(state,previous_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the logits to probabilities\n",
    "torch.softmax(net(state,previous_state),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c00f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picks an action and report its probability\n",
    "net.sample_action(state,previous_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6399be0",
   "metadata": {},
   "source": [
    "## DataLoader and PolicyLoss\n",
    "\n",
    "At every step of the loop I save the current state, the previous state, the action, its probability and the reward I get. I let the game playing sometimes without training the network but saving all the actions and rewards.\n",
    "\n",
    "After the end of the game I give a delayed reward (reward knowing the future). The idea is to give a positive reward (that decays in time) to all the actions the allowed us to get a point. Similarly, I give a negative reward to all the actions the brought us to loosing!\n",
    "\n",
    "The loss function will have as an input the state, action, probability of the action, reward and delayed reward. We look at the action and delayed reward, if the reward is positive I want to increase the action probability (and viceversa).\n",
    "Defining $a$ as action and $dr$ as delayed reward\n",
    "\n",
    "\\begin{equation}\n",
    "- \\frac{P_{new} \\left( a \\right)}{P_{old} \\left( a \\right)} \\cdot dr  \n",
    "\\end{equation}\n",
    "\n",
    "where $P_{new}$ is the probability of action take now by the NN and $P_{old}$ is the probability during the beginning games. We divide by the old probability in order to not allowing a drastic change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import GamesMemoryBank\n",
    "from policy_loss import PolicyLoss\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364804f6",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PolicyNetwork()\n",
    "loss_func = PolicyLoss()\n",
    "memory_bank = GamesMemoryBank()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8624ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3c2feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('trained_model.pt'):\n",
    "    n_epochs = 100\n",
    "    games_per_epoch = 10\n",
    "    steps_per_game = 190000\n",
    "    batch_size = 24000\n",
    "    num_batches = 5\n",
    "\n",
    "    points_scored_per_game = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # We clear the memory bank\n",
    "        memory_bank.clear_memory()\n",
    "\n",
    "        # First part is about letting the agent play and store all the action it takes\n",
    "        net.eval()\n",
    "        net.cpu()\n",
    "\n",
    "        for game_i in tqdm( range(games_per_epoch) ):\n",
    "\n",
    "            state, previous_state = env.reset(), None\n",
    "\n",
    "            state = preprocess(state).view(-1).unsqueeze(0)\n",
    "            previous_state = preprocess(previous_state).view(-1).unsqueeze(0)\n",
    "\n",
    "            points_in_game = 0\n",
    "\n",
    "            for t in range(steps_per_game):\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    action, action_prob = net.sample_action(state,previous_state)\n",
    "\n",
    "                new_state, reward, done, info = env.step(action+2) # +2 is because in the set of actions left and right are idx 2 and 3\n",
    "\n",
    "                memory_bank.add_event(...)\n",
    "\n",
    "                previous_state = state\n",
    "                state = ...\n",
    "\n",
    "                if reward > 0:\n",
    "                    points_in_game+=1\n",
    "\n",
    "                if done:\n",
    "                    points_scored_per_game.append(points_in_game)\n",
    "                    break\n",
    "\n",
    "        # We compute the rewards based on the history of actions\n",
    "        memory_bank.compute_reward_history()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.title('epoch '+ str(epoch) + ', mean points per last 10 games ' + str(np.mean(points_scored_per_game[-10:])))\n",
    "        plt.plot(points_scored_per_game)\n",
    "        plt.xlim(0,1000)\n",
    "        plt.ylim(-1,21)\n",
    "        plt.xlabel('n_epochs')\n",
    "        plt.ylabel('points scored')\n",
    "        plt.show()\n",
    "\n",
    "        # Training phase\n",
    "        net.train()\n",
    "\n",
    "        for batch_i in range(num_batches):\n",
    "\n",
    "            optimizer...\n",
    "\n",
    "            state, previous_state, action, action_prob, reward, discounted_reward = memory_bank.get_sample(batch_size)\n",
    "\n",
    "            # Be careful of the shape\n",
    "            logits = ...\n",
    "            loss = ...\n",
    "\n",
    "            loss...\n",
    "            optimizer..\n",
    "\n",
    "        torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a3d1e",
   "metadata": {},
   "source": [
    "### Another game :)\n",
    "\n",
    "I want to see the improvements made by the neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230979",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('trained_model.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, previous_state = env.reset(), None\n",
    "\n",
    "screens = []\n",
    "\n",
    "state = preprocess(state).view(-1).unsqueeze(0)\n",
    "previous_state = preprocess(previous_state).view(-1).unsqueeze(0)\n",
    "\n",
    "for t in range(190000):\n",
    "    \n",
    "    action, action_prob = net.sample_action(state,previous_state)\n",
    "\n",
    "    new_state, reward, done, info = env.step(action+2)\n",
    "    \n",
    "    next_state, reward, done, info = env.step(env.action_space.sample())\n",
    "    screens.append(next_state)\n",
    "    \n",
    "    previous_state = state\n",
    "    state = preprocess(new_state).view(-1).unsqueeze(0)\n",
    "\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ims = []\n",
    "\n",
    "for i, screen in enumerate( screens ):\n",
    "    if i % 5 !=0:\n",
    "        continue\n",
    "    im = ax.imshow(screen,animated=True)\n",
    "    if i == 0:\n",
    "        ax.imshow(screen)\n",
    "    ims.append([im])\n",
    "\n",
    "    if i > 5000:\n",
    "        break\n",
    "\n",
    "ani_results = animation.ArtistAnimation(fig, ims, interval=20, blit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08cc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(ani_results.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537a639e",
   "metadata": {},
   "source": [
    "The performance clearly increased, but we can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c684885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
