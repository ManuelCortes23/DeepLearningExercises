{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab\n",
    "\n",
    "#!pip install dgl-cu100\n",
    "#!pip install scipy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44354776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b38001",
   "metadata": {},
   "source": [
    "### Homework 4: Attention Mechanisms\n",
    "\n",
    "This is a key exercise for learning transformers, but in this case we will do it with graph neural networks.\n",
    "\n",
    "The goals are:\n",
    "\n",
    "1. Learn about heterogeneous graphs in DGL (graphs with multiple types of nodes and edges)\n",
    "2. Implement key - query attention\n",
    "3. Learn about slot attention and permutation invariant loss\n",
    "\n",
    "<b> The task is object detection around a cloud of points. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f574b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d54c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Dataset.zip'):\n",
    "    !wget https://www.dropbox.com/s/qrivkcb50yliez9/Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1950e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Dataset'):\n",
    "    !unzip Dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ff6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already implemented\n",
    "\n",
    "from dataloader import RandomShapeDataset, collate_graphs,plot_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomShapeDataset('Dataset/training.bin')\n",
    "validation_ds = RandomShapeDataset('Dataset/validation.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078d7ba",
   "metadata": {},
   "source": [
    "Your input information is a set of points and their positions. \n",
    "\n",
    "You want to identify how many clusters there are and for each cluster you want to draw a box around it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target for training\n",
    "fig,ax = plt.subplots(4,4,figsize=(8,8),dpi=100)\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax_i = ax[i][j]\n",
    "        g = validation_ds[np.random.randint(len(validation_ds))]\n",
    "        plot_graph(g,ax_i,size=0.2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8296262",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c95863",
   "metadata": {},
   "source": [
    "#### How is this represented on our graph?\n",
    "\n",
    "1. Each node store a dictionary with (objects, points and predicted objects)\n",
    "2. You have different edges (the points to predicted object, the predicted objects to target)\n",
    "\n",
    "<img src=\"structure.jpeg\" width=\"800\" height=\"400\">\n",
    "\n",
    "The points are the blue cloud, the objects are your target. Each bounding box is represented by four numbers (2 coordinates for the center, height and width of the box). \n",
    "\n",
    "The predicted objects are element of the graph where we will store our prediction and afterwards compare it to the target object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96493b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=300, shuffle=True,\n",
    "                         collate_fn=collate_graphs)\n",
    "\n",
    "valid_data_loader = DataLoader(validation_ds, batch_size=300, shuffle=False,\n",
    "                         collate_fn=collate_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batched_g in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574503ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of clusters (you need to specify what tiy want to see)\n",
    "batched_g.batch_num_nodes('objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2bdd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since it's an heterogeneous graph we need to specify which nodes are we dealing with \n",
    "#Before we were using batched_g.ndata[...], now this is not possible, because we have an hetereogenous graph!\n",
    "batched_g.nodes['points'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7612a",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "It is based on Object-Centric Learning with Slot Attention, https://arxiv.org/abs/2006.15055.\n",
    "\n",
    "We start with an array of points. First, we want to pass these through a DeepSet (like exercise 3, part 1). This will produce a global representation for the graph and hidden representation for the nodes.\n",
    "\n",
    "On one side, we use this global representation to make a prediction on how many objects (clusters) there are in the cloud of points (classification problem, 2/3/4 objects). On the other side, we ignore the set size prediction and we CHEAT during the training. We initialize the predicted objects to be the same amount of the real objects. \n",
    "\n",
    "Then we have a slot attention part (figure below, with key, value and query). The key and value come from the points, while the query come from the objects. We do the dot product of the key and the query. We do a weighted sum of the values for each one of the objects. We put all of this through a GRU cell (recurrent network). \n",
    "\n",
    "This creates an updated hidden representation of the predicted objects that captures more features about our data.\n",
    "\n",
    "The last part is a simple FC network to predict the box boundaries (center, width and height)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a65f70",
   "metadata": {},
   "source": [
    "<img src=\"model_1.jpeg\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1d428",
   "metadata": {},
   "source": [
    "<img src=\"model_2.jpeg\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056612ec",
   "metadata": {},
   "source": [
    "After running the DeepSet you have to:\n",
    "\n",
    "1. Create the size prediction\n",
    "2. Create the object prediction (center, width and height) in case of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a1b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batched_g in data_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db669ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted graph and prediction of how many clusters there are..\n",
    "predicted_g, size_pred = net(batched_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a29dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_g, size_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fc1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_g.nodes['objects'].datata['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_g.nodes['predicted objects'].data['properties'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82af545",
   "metadata": {},
   "source": [
    "## Permutation invariant loss\n",
    "\n",
    "We need to compute two different losses in order to train our network:\n",
    "\n",
    "1. The loss for the object boundaries\n",
    "\n",
    "    https://en.wikipedia.org/wiki/Hungarian_algorithm\n",
    "\n",
    "    The loss computation has to take into account the fact that there is no order to the output. I can predict the objects boundaries in whatever order I want, and the loss should not be affected by this.\n",
    "    \n",
    "\n",
    "2. The loss for the size prediction (a simple CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already implemented.. have a look!\n",
    "\n",
    "from loss import Set2SetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Set2SetLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1892b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func(batched_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9188e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss for the size prediction, a classical classification task\n",
    "\n",
    "size_loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f8dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_loss_func(size_pred, batched_g.batch_num_nodes('objects')-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f9ed7",
   "metadata": {},
   "source": [
    "### Training the objects prediction\n",
    "\n",
    "The idea is to first train the bounding boxes, since they do not care about the size prediction (we give it to the network). \n",
    "\n",
    "Aftwerwards, we will freeze all these weights and train only the size prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc31c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e27aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomShapeDataset('Dataset/training.bin')\n",
    "validation_ds = RandomShapeDataset('Dataset/validation.bin')\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=300, shuffle=True,\n",
    "                         collate_fn=collate_graphs)\n",
    "\n",
    "valid_data_loader = DataLoader(validation_ds, batch_size=300, shuffle=False,\n",
    "                         collate_fn=collate_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.0005) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c976add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42c50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_vs_epoch = []\n",
    "validation_loss_vs_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103e070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I run it on colab\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    n_epochs = 400 #it takes a while.. like 2 hours!\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "\n",
    "        if len(validation_loss_vs_epoch) > 0:\n",
    "\n",
    "            print(epoch, 'train loss',training_loss_vs_epoch[-1],'validation loss',validation_loss_vs_epoch[-1])\n",
    "\n",
    "        net.train() # put the net into \"training mode\"\n",
    "\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        for batched_g in tqdm(data_loader):\n",
    "            n_batches+=1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                batched_g = batched_g.to(torch.device('cuda'))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_g,size_pred = net(batched_g)\n",
    "\n",
    "            loss = loss_func(batched_g) \n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = epoch_loss/n_batches\n",
    "        training_loss_vs_epoch.append(epoch_loss)\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            for batched_g in tqdm(valid_data_loader):\n",
    "                n_batches+=1\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    batched_g = batched_g.to(torch.device('cuda'))\n",
    "\n",
    "                predicted_g,size_pred = net(batched_g,use_target_size=True)\n",
    "\n",
    "                loss = loss_func(batched_g) \n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss/n_batches\n",
    "            validation_loss_vs_epoch.append(epoch_loss)\n",
    "\n",
    "        if len(validation_loss_vs_epoch)==1 or np.amin(validation_loss_vs_epoch[:-1]) > validation_loss_vs_epoch[-1]:\n",
    "            torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "\n",
    "    plt.plot(training_loss_vs_epoch)\n",
    "    plt.plot(validation_loss_vs_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp trained_model.pt trained_model_objects.pt #making a copy in case something goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd09a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('trained_model.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f60c85",
   "metadata": {},
   "source": [
    "### Results of first training \n",
    "\n",
    "I check the results without training the size prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net.cpu()\n",
    "predicted_sizes = []\n",
    "for batched_g in valid_data_loader:\n",
    "    predicted_g,size_pred = net(batched_g)\n",
    "    \n",
    "    predicted_sizes+=list(torch.argmax(size_pred,dim=1).cpu().data.numpy())\n",
    "    \n",
    "predicted_sizes = np.array(predicted_sizes)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a859a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = np.array([validation_ds[i].num_nodes('objects') for i in range(len(validation_ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not training the size prediction yet\n",
    "# In this plot you will see how the predicted labels differ from the true labels\n",
    "\n",
    "cm = confusion_matrix(target_sizes, predicted_sizes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['2','3','4'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dcc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I randomly select a validation graph to check (you can change this)\n",
    "\n",
    "idxValidation = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c573c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = validation_ds[idxValidation].cpu()\n",
    "\n",
    "net.eval()\n",
    "predicted_g,size_pred = net(g) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea71e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_g.num_nodes('predicted objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3305d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = g.nodes['points'].data['xy'][:,0].data.numpy()\n",
    "y = g.nodes['points'].data['xy'][:,1].data.numpy()\n",
    "object_centers = g.nodes['objects'].data['centers'].data.numpy()\n",
    "\n",
    "object_width = g.nodes['objects'].data['width'].data.numpy()\n",
    "object_height = g.nodes['objects'].data['height'].data.numpy()\n",
    "\n",
    "predicted_heights = predicted_g.nodes['predicted objects'].data['properties'][:,0].data.numpy()\n",
    "predicted_widths = predicted_g.nodes['predicted objects'].data['properties'][:,1].data.numpy()\n",
    "predicted_centers = predicted_g.nodes['predicted objects'].data['properties'][:,[2,3]].data.numpy()\n",
    "\n",
    "attn_weights = predicted_g.edges['points_to_object'].data['attention weights'].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e91847",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = predicted_g.num_nodes('points')\n",
    "n_objects = predicted_g.num_nodes('predicted objects')\n",
    "n_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d776e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "estart = predicted_g.edges(etype='points_to_object')[0].cpu().data.numpy()\n",
    "eend =  predicted_g.edges(etype='points_to_object')[1].cpu().data.numpy()\n",
    "\n",
    "weight_dict = {i:{} for i in range(n_objects)}\n",
    "for e_i,(es,ee) in enumerate(zip(estart,eend)):\n",
    "    weight_dict[ee][es] = attn_weights[e_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c32c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,n_objects,figsize=(3*n_objects,6),dpi=100)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[0][i].scatter(x,y,c='cornflowerblue',cmap='tab10',s=3)\n",
    "\n",
    "ax[0][1].scatter(predicted_centers[:,0],predicted_centers[:,1],c='r',cmap='tab10',s=30,ec='k')\n",
    "ax[0][0].scatter(object_centers[:,0],object_centers[:,1],c='r',marker='o',s=30,ec='k')\n",
    "\n",
    "for i in range(len(object_height)):\n",
    "    \n",
    "    bounding_box = patches.Rectangle((object_centers[i][0]-object_width[i]/2, object_centers[i][1]-object_height[i]/2), \n",
    "                             object_width[i], object_height[i], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "    ax[0][0].add_patch(bounding_box) \n",
    "    \n",
    "for i in range(len(predicted_centers)):\n",
    "    \n",
    "    bounding_box = patches.Rectangle((predicted_centers[i][0]-predicted_widths[i]/2, \n",
    "                                          predicted_centers[i][1]-predicted_heights[i]/2), \n",
    "                             predicted_widths[i], predicted_heights[i], linewidth=1, \n",
    "                                         edgecolor='darkgreen', facecolor='none')\n",
    "\n",
    "    ax[0][1].add_patch(bounding_box)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[0][i].set_xlim(-1,1)\n",
    "    ax[0][i].set_ylim(-1,1)\n",
    "\n",
    "for object_idx in range(n_objects):\n",
    "    object_attn_weights = []\n",
    "\n",
    "    for point_i in range(n_points):\n",
    "        object_attn_weights.append(weight_dict[object_idx][point_i])\n",
    "\n",
    "\n",
    "    object_attn_weights = torch.softmax(torch.tensor(object_attn_weights),dim=0).data.numpy()\n",
    "\n",
    "    ax[1][object_idx].scatter(x,y,s=0.2,alpha=0.2)\n",
    "    ax[1][object_idx].scatter(x,y,s=300.0*object_attn_weights,alpha=0.8,c=object_attn_weights,cmap='Reds')\n",
    "\n",
    "    ax[1][object_idx].set_xlim(-1,1)\n",
    "    ax[1][object_idx].set_ylim(-1,1)\n",
    "    \n",
    "    bounding_box = patches.Rectangle((predicted_centers[object_idx][0]-predicted_widths[object_idx]/2, \n",
    "                                          predicted_centers[object_idx][1]-predicted_heights[object_idx]/2), \n",
    "                             predicted_widths[object_idx], predicted_heights[object_idx], linewidth=1, \n",
    "                                         edgecolor='r', facecolor='none')\n",
    "\n",
    "\n",
    "    ax[1][object_idx].scatter(predicted_centers[:,0][object_idx],\n",
    "                           predicted_centers[:,1][object_idx],c='r',marker='o',s=30,ec='k')\n",
    "    ax[1][object_idx].add_patch(bounding_box)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af46c8",
   "metadata": {},
   "source": [
    "If you did everything correct, the model will be able to create correctly boxes around the different clusters. We can notice that, since the size prediction has not been trained, the number of clusters found is wrong.\n",
    "\n",
    "The top left plot corresponds to our target, while the top right plot to our prediction.\n",
    "The bottom plots reflect the slot attention mechanism. Each of the predicted objects should pay attention to the part of points within the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7219b3",
   "metadata": {},
   "source": [
    "### Training the size prediction\n",
    "\n",
    "Now I can freeze everything and only focus on training the size prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_vs_epoch = []\n",
    "validation_loss_vs_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RandomShapeDataset('Dataset/training.bin')\n",
    "validation_ds = RandomShapeDataset('Dataset/validation.bin')\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=300, shuffle=True,\n",
    "                         collate_fn=collate_graphs)\n",
    "\n",
    "valid_data_loader = DataLoader(validation_ds, batch_size=300, shuffle=False,\n",
    "                         collate_fn=collate_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4360950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I loop over the network, and unless is 'size_predictor', I freeze the weights\n",
    "\n",
    "for p_name, p in net.named_parameters():\n",
    "    if 'size_predictor' not in p_name:\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.size_predictor.parameters(), lr=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8970ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    \n",
    "    n_epochs = 180\n",
    "\n",
    "    for epoch in range(n_epochs): \n",
    "\n",
    "        if len(validation_loss_vs_epoch) > 0:\n",
    "\n",
    "            print(epoch, 'train loss',training_loss_vs_epoch[-1],'validation loss',validation_loss_vs_epoch[-1])\n",
    "\n",
    "        net.train() # put the net into \"training mode\"\n",
    "\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        for batched_g in tqdm(data_loader):\n",
    "            n_batches+=1\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                batched_g = batched_g.to(torch.device('cuda'))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicted_g,size_pred = net(batched_g)\n",
    "\n",
    "\n",
    "            loss = size_loss_func(size_pred, batched_g.batch_num_nodes('objects')-2 )\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss = epoch_loss/n_batches\n",
    "        training_loss_vs_epoch.append(epoch_loss)\n",
    "\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            for batched_g in tqdm(valid_data_loader):\n",
    "                n_batches+=1\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    batched_g = batched_g.to(torch.device('cuda'))\n",
    "\n",
    "                predicted_g,size_pred = net(batched_g,use_target_size=True)\n",
    "\n",
    "                loss = size_loss_func(size_pred, batched_g.batch_num_nodes('objects')-2 )\n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "            epoch_loss = epoch_loss/n_batches\n",
    "            validation_loss_vs_epoch.append(epoch_loss)\n",
    "\n",
    "        if len(validation_loss_vs_epoch)==1 or np.amin(validation_loss_vs_epoch[:-1]) > validation_loss_vs_epoch[-1]:\n",
    "            torch.save(net.state_dict(), 'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    \n",
    "    plt.plot(training_loss_vs_epoch)\n",
    "    plt.plot(validation_loss_vs_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef284ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.cpu()\n",
    "net.load_state_dict(torch.load('trained_model.pt',map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe2a13",
   "metadata": {},
   "source": [
    "### Results with everything trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "net.cpu()\n",
    "predicted_sizes = []\n",
    "for batched_g in valid_data_loader:\n",
    "    predicted_g,size_pred = net(batched_g)\n",
    "    \n",
    "    predicted_sizes+=list(torch.argmax(size_pred,dim=1).cpu().data.numpy())\n",
    "    \n",
    "predicted_sizes = np.array(predicted_sizes)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sizes = np.array([validation_ds[i].num_nodes('objects') for i in range(len(validation_ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4dc181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(target_sizes, predicted_sizes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['2','3','4'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = validation_ds[idxValidation].cpu()\n",
    "\n",
    "net.eval()\n",
    "predicted_g, size_pred = net(g) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_g.num_nodes('predicted objects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b8c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = g.nodes['points'].data['xy'][:,0].data.numpy()\n",
    "y = g.nodes['points'].data['xy'][:,1].data.numpy()\n",
    "object_centers = g.nodes['objects'].data['centers'].data.numpy()\n",
    "\n",
    "object_width = g.nodes['objects'].data['width'].data.numpy()\n",
    "object_height = g.nodes['objects'].data['height'].data.numpy()\n",
    "\n",
    "predicted_heights = predicted_g.nodes['predicted objects'].data['properties'][:,0].data.numpy()\n",
    "predicted_widths = predicted_g.nodes['predicted objects'].data['properties'][:,1].data.numpy()\n",
    "predicted_centers = predicted_g.nodes['predicted objects'].data['properties'][:,[2,3]].data.numpy()\n",
    "\n",
    "attn_weights = predicted_g.edges['points_to_object'].data['attention weights'].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = predicted_g.num_nodes('points')\n",
    "n_objects = predicted_g.num_nodes('predicted objects')\n",
    "n_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e11ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estart = predicted_g.edges(etype='points_to_object')[0].cpu().data.numpy()\n",
    "eend =  predicted_g.edges(etype='points_to_object')[1].cpu().data.numpy()\n",
    "\n",
    "weight_dict = {i:{} for i in range(n_objects)}\n",
    "for e_i,(es,ee) in enumerate(zip(estart,eend)):\n",
    "    weight_dict[ee][es] = attn_weights[e_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2b34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,n_objects,figsize=(3*n_objects,6),dpi=100)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[0][i].scatter(x,y,c='cornflowerblue',cmap='tab10',s=3)\n",
    "\n",
    "ax[0][1].scatter(predicted_centers[:,0],predicted_centers[:,1],c='r',cmap='tab10',s=30,ec='k')\n",
    "ax[0][0].scatter(object_centers[:,0],object_centers[:,1],c='r',marker='o',s=30,ec='k')\n",
    "\n",
    "for i in range(len(object_height)):\n",
    "    \n",
    "    bounding_box = patches.Rectangle((object_centers[i][0]-object_width[i]/2, object_centers[i][1]-object_height[i]/2), \n",
    "                             object_width[i], object_height[i], linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "\n",
    "    ax[0][0].add_patch(bounding_box) \n",
    "    \n",
    "for i in range(len(predicted_centers)):\n",
    "    \n",
    "    bounding_box = patches.Rectangle((predicted_centers[i][0]-predicted_widths[i]/2, \n",
    "                                          predicted_centers[i][1]-predicted_heights[i]/2), \n",
    "                             predicted_widths[i], predicted_heights[i], linewidth=1, \n",
    "                                         edgecolor='darkgreen', facecolor='none')\n",
    "\n",
    "    ax[0][1].add_patch(bounding_box)\n",
    "\n",
    "for i in range(2):\n",
    "    ax[0][i].set_xlim(-1,1)\n",
    "    ax[0][i].set_ylim(-1,1)\n",
    "\n",
    "for object_idx in range(n_objects):\n",
    "    object_attn_weights = []\n",
    "\n",
    "    for point_i in range(n_points):\n",
    "        object_attn_weights.append(weight_dict[object_idx][point_i])\n",
    "\n",
    "    object_attn_weights = torch.softmax(torch.tensor(object_attn_weights),dim=0).data.numpy()\n",
    "\n",
    "    ax[1][object_idx].scatter(x,y,s=0.2,alpha=0.2)\n",
    "    ax[1][object_idx].scatter(x,y,s=300.0*object_attn_weights,alpha=0.8,c=object_attn_weights,cmap='Reds')\n",
    "\n",
    "    ax[1][object_idx].set_xlim(-1,1)\n",
    "    ax[1][object_idx].set_ylim(-1,1)\n",
    "    \n",
    "    bounding_box = patches.Rectangle((predicted_centers[object_idx][0]-predicted_widths[object_idx]/2, \n",
    "                                          predicted_centers[object_idx][1]-predicted_heights[object_idx]/2), \n",
    "                             predicted_widths[object_idx], predicted_heights[object_idx], linewidth=1, \n",
    "                                         edgecolor='r', facecolor='none')\n",
    "\n",
    "\n",
    "    ax[1][object_idx].scatter(predicted_centers[:,0][object_idx],\n",
    "                           predicted_centers[:,1][object_idx],c='r',marker='o',s=30,ec='k')\n",
    "    ax[1][object_idx].add_patch(bounding_box)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83d129",
   "metadata": {},
   "source": [
    "Now the model should correctly perform the task!!!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b52e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_homework import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_homework()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
